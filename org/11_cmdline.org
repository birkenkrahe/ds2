#+TITLE:Data science on the command line
#+AUTHOR:Marcus Birkenkrahe
#+SUBTITLE:Introduction to advanced data science
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:bash :exports both :results output
* README

Short introduction to doing data science on the command line:
- What is the command line?
- 

- Reminder of the data science pipeline

  #+attr_html: :width 500px
  #+caption: Data science pipeline (Janssen, 2021)
  [[../img/11_pipeline.png]]

* What is the command line?

  - Shell
  - bash, csh, sh, zsh, ksh
  - CMD line
  - Terminal
  - tty

    #+attr_html: :width 500px
    #+caption: Command line terminal (bash)
    [[../img/11_bash.png]]

    | CMD LINE TOOL      | EXAMPLE         |
    |--------------------+-----------------|
    | Binary executable  | ~bash --help~     |
    | Shell builtin      | ~cd .~            |
    | Interpreted script | ~hello.sh~        |
    | Shell function     | ~pwd~, ~date~, ~echo~ |
    | Alias              | ~alias~         |


  #+attr_html: :width 500px
  #+caption: Things to do on the command line (Source: Janssens, 2021)
  [[../img/11_cmdline.png]]

* Why use the command line for data science

1. Program to interact with the operating system (kernel)
2. Sophisticated script language
3. REPL (Read-Eval-Print-Loop)
4. Agile and exploratory
5. Augmenting technology (glue to other applications)
   - Run pipeline (e.g. ~ls -a | wc -w~)
   - Run from inside your R program
   - Convert R code to command line script
6. Scalability:
   - it's fast
   - used to automate tasks
   - repeatable and parallelizable
7. Extensibility:
   - language agnostic
   - been in use a long time
   - continuously improved
8. Ubiquitous: comes with all OS
9. Cool (hacker value)
10. Relatable (logical approach)

    All of these are especially valuable in an exploratory environment
    with highly distributed, unstructured, or "dirty" data sources.

* How to get a command line that works for data science

- Install a Docker container as described [[https://github.com/birkenkrahe/org/blob/master/FAQ.org#how-to-set-up-a-docker-container-for-command-line-work][in this FAQ]]
- Install the Ubuntu app as [[https://github.com/birkenkrahe/org/blob/master/FAQ.org#how-can-i-install-linux-under-windows-10][described in this FAQ]]
- Get Linux as a dual boot or with VirtualBox (any distro)
- Get a Linux computer ([[https://vilros.com/products/raspberry-pi-400-kit][like this one for $99]]) or dump Windows for
  Linux and install it over Windows

  Online/cloud installations like Google cloud shell, or replit.com,
  or the bundle of UNIX commands contained in ~cygwin~ do unfortunately
  not allow you to install the ~csvkit~ library, and exclude some other
  commands (like ~wget~).

  - The Docker container already comes with ~cvskit~. Once you've got
    another Linux variant, install ~cvskit~ from the command line,
    e.g. in Debian-based systems (Raspberry Pi OS, Ubuntu) with the
    command ~sudo apt install csvkit~.

* csvkit (interactive demo)

This demo is only interactive if you have access to a Linux
installation of some sort. I'm using the ~csvkit~ [[https://csvkit.readthedocs.io/en/latest/tutorial.html][online tutorial]].

~csvskit~ is a Python-3 library for manipulating text files.

* Install ~csvkit~

- ~pip~ is a python-based package manager tool.

  #+begin_example bash
 sudo pip install csvkit
  #+end_example

- Or perhaps you already have all the ~csv*~ tools? The output should
  show a bunch of different executables.

  #+begin_src bash
    ls -l /usr/bin/*csv*
  #+end_src

- If ~pip~ is missing, you can install it

  #+begin_example bash
 sudo apt install pip
  #+end_example

* Getting data

- Make a new working directory
- Change into it
- Check where you are

  #+begin_example bash
 mkdir -v csvkit_tutorial
 cd csvkit_tutorial
 pwd
  #+end_example

- Fetch the data with ~curl(1)~ - check if you got it with ~which curl~,
  otherwise install it with ~sudo apt install curl~.

  #+begin_example bash
  url="https://raw.githubusercontent.com/wireservice/csvkit/master/examples/realdata/ne_1033_data.xlsx"
  curl --location --remote-name $url
  #+end_example

  The short version of the ~curl~ options: ~-L -O~

- Check if the file ~.xlsx~ file is there - the ~file~ command gives
  you some file type information.

  #+begin_example bash
 file *
  #+end_example

- You can also try to get any old HTML file, like from Lyon:

  #+begin_example bash
 curl https://lyon.edu | tee fetched | head
 file fetched
  #+end_example
* Look at the data
** ~in2csv~ to re-write an Excel file as CSV file

- Excel is a binary format - you cannot look at it (without paying
  Microsoft).

- ~in2csv~ rewrites the Excel file into CSV. Btw, long file names can
  be expanded using the <TAB> key.

  #+begin_example bash
 in2csv ne_1033_data.xlsx > data.csv 2&>/dev/null
 head -5 data.csv
  #+end_example

  - ~in2csv~ runs the conversion on the following file
  - ~> data.csv~ redirects the result to a file ~data.csv~
  - ~2&>/dev/null~ throws standard error msgs away

** ~csvlook~ to get a table output of the CSV file
- ~csvlook~ provides a tabular look at the data.

  #+begin_example bash
  csvlook --max-rows 5 data.csv
  #+end_example

** ~csvcut~ to cut out columns from a CSV file
- ~csvcut~ is a version of ~cut~ for ~CSV~ files
  - the ~-n~ option shows all columns
  - the ~-c~ option shows specific columns

    #+begin_example bash
   csvcut -n data.csv
   csvcut -c 2,5,6 | head -5
    #+end_example

  - output columns can be called by name, too

    #+begin_example bash
   csvcut -c county, item_name, quantity data.csv | csvlook | head -5
    #+end_example

  - the pipe prints the first 5 rows of the respective columns
  - I want to use some of the output later so I put it into a file:

    #+begin_example bash
   csvcut -c county, item_name, quantity data.csv | tee data1.csv | csvlook | head -5
    #+end_example

- All of the previous operations can be put together in one pipe:

  #+begin_example bash
 in2csv ne_1033_data.xlsx 2&>/dev/null |
 csvcut -c county,item_name,quantity |
 csvlook |
 head -5
  #+end_example

* Examining data
** ~csvstat~ for summary statistics

- ~csvstat~ is inspired by R's ~summary~ function

  #+begin_example bash
   csvstat data1.csv
  #+end_example

  or

  #+begin_example bash
 cat data1.csv | csvstat
  #+end_example

** ~csvgrep~ for matching patterns in the file

- ~csvgrep~ is a pattern-matching search function.
  - run ~csvgrep~ on the ~data1.csv~ subset
  - focus on the ~county~ column with ~-c~
  - match the pattern ~LANCASTER~ county
  - look at the result as a table

    #+begin_example bash
   csvgrep -c county -m LANCASTER data1.csv | csvlook
    #+end_example

  - count the lines (= entries for LANCASTER county)

    #+begin_example bash
   csvgrep -c county -m LANCASTER data1.csv | wc -l
    #+end_example

** ~csvsort~ to sort rows by column

- ~csvsort~ sorts the rows by any column (or combination of columns)
  in ascending or descending (reverse) order.

  #+begin_example bash
 csvcut -c county,item_name,total_cost data.csv > data2.csv
 cat data2.csv | csvgrep -c county -m LANCASTER > data3.csv
 cat data3.csv | csvsort -c total_cost -r | csvlook
  #+end_example

* Beyond csvkit

- Before this term, I had not worked with csvkit myself. My work on
  the command line was limited to the UNIX commands that I know
  (which by themselves are pretty powerful, at least for
  non-descript text files).

- To go beyond ~csvkit~, you need to look beyond data scrubbing (which
  is where ~csvkit~ excels), into the other parts of the data science
  pipeline: visualization (on the command line with the R package
  ~rush~), modeling, and presenting, managing projects with GNU ~make~
  and UNIX ~cron~, R scripting and running scripts with ~littler~ and
  GNU ~rush~.

- Of course, R has a console that is already tuned to interactive
  explorative use - but it's slow, especially if you're interested
  in routine big data processing, and if you don't want to write
  your own programs (e.g. in C++ and import them into R with ~Rcpp~).

- The new edition of DSC 105 (Introduction to data science - tools
  and methods) will feature a block on doing "data science on the
  command line", and the new edition of DSC 205 (Introduction to
  advanced data science) will hopefully deepen this topic further.

- Links:
  - [[http://jeroenjanssens.github.io/rush/][R rush]] - run expressions, create plots etc. from the shell
  - [[https://cran.r-project.org/web/packages/littler/index.html][R littler]] - command line script support
  - [[https://puszcza.gnu.org.ua/software/rush/][GNU rush]] - reduced shell for parallel execution
  - [[https://www.gnu.org/software/make/][GNU make]] - generate executables from source
  - [[https://swcarpentry.github.io/r-novice-inflammation/05-cmdline/index.html][R on the command line]] - tutorial (30 min)
  - [[https://app.datacamp.com/learn/courses/data-processing-in-shell][Data processing in shell]] - DataCamp course (4 hrs)
  - [[https://rolkra.github.io/regex-for-beginners-detect/][Brief introduction to regular expressions in R]] for beginners

    You should remember these functions:
    #+begin_src R :exports both :session :results output
      grep("Orange", c("Orange", "Apple","Lemon","Orange"))
    #+end_src

    #+RESULTS:
    : [1] 1 4

    #+begin_src R :exports both :session :results output
      grepl("Orange", c("Orange", "Apple","Lemon","Orange"))
    #+end_src

    #+RESULTS:
    : [1]  TRUE FALSE FALSE  TRUE

* References

Janssens (2021). Data science at the command line (2e). O'Reilly.
